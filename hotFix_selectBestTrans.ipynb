{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hot fix for function in civiActivityReport_selectBestTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import itertools\n",
    "from datetimerange import DateTimeRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original function; works some of the time, but there are cases where it doesn't\n",
    "def intake_file(activityReport):\n",
    "\n",
    "    #I removed some data munging here, as this should be done in another module or the calling ipynb; munging removed denoted w/ \"*\"\n",
    "\n",
    "    #activityReport.columns = [i.replace(' ','_')+'_act' for i in list(activityReport.columns)]*\n",
    "\n",
    "    #trim all string fields\n",
    "    #strip all whitespace from each cell\n",
    "    #activityReport = activityReport.map(lambda x: x.strip() if isinstance(x,str) else x) *\n",
    "\n",
    "    #NOTE: Activity_Date_act field DOES NOT provide seconds\n",
    "    #activityReport = activityReport.assign(Activity_Date_DT_act = pd.to_datetime(activityReport['Activity_Date_act'], format = '%Y-%m-%d %H:%M')) *\n",
    "\n",
    "    try:\n",
    "        activityReport = activityReport.drop_duplicates(ignore_index = True, subset = ['Activity_Type_act', 'Subject_act', 'Activity_Date_act', 'Activity_Status_act', 'Activity_Date_DT_act','Target_Email_act'])\n",
    "\n",
    "        # ### SELECT BEST TRANSACTION LOGIC\n",
    "        #NOTE: real dupes have multiple entries on fields: 'Target_Email_act','Activity_Date_DT_act','Activity_Type_act'\n",
    "        #NOTE 'Activity_Type_act' = specifies if change is for status or type\n",
    "        # looks like the index is preserved on a groupby\n",
    "        df_grouped = activityReport.groupby(['Target_Email_act','Activity_Date_DT_act','Activity_Type_act']).filter(lambda x: len(x) > 1)\n",
    "\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f'fields in this version of the dataframe do not coincide with the code in civiActivityReport_selectBestTrans.py: {e}')\n",
    "\n",
    "\n",
    "    #assign a row value to ea group member: used later for selection\n",
    "    df_grouped['count'] = df_grouped.groupby(['Target_Email_act','Activity_Date_DT_act','Activity_Type_act']).cumcount()+1\n",
    "\n",
    "\n",
    "    # Issues to handle:\n",
    "    # - multiple records for different versions of Target_Name_act (yet Target_Email_act is the same) <- will need to check that this doesn't delete Family account members\n",
    "    # - records made by different Source_Email_act ie systematic records made by CIVI\n",
    "\n",
    "    df_grouped['from'] = df_grouped['Subject_act'].str.extract(r'from\\s(\\w+)') #str.extract(r'from\\s(\\w+)')\n",
    "    df_grouped['to'] = df_grouped['Subject_act'].str.extract(r'to\\s(\\w+)') #str.extract(r'to\\s(\\w+)')\n",
    "\n",
    "\n",
    "    # The record to keep is that where the \"From\" = \"To\" of the companion line of the group. There are only at most two entries with the same Start_dt, so I only need to worry about passing back the derived \"To\" and \"From\" fields twice. Either of row = 1 or row = 2 can be the best one to keep. Essentially I'm searching for the best determinite record of the status going forward, ignoring the journey (implying that accurate status is more important than accurate/comprehensive journey).\n",
    "    # \n",
    "    # Will need to offset forwards and backwards. Ea row will then have a pair of offset values (from the row above and the one beneath). Depending on the \"count\" value (ie 1,2), only one from the pair will be relevant and tested to determine the \"best\" and \"final\" value.\n",
    "\n",
    "    df_grouped.sort_values(['Target_Email_act','Activity_Type_act','Activity_Date_DT_act'],inplace= True)\n",
    "    # -1 = the row below (lead); +1 = the row above (lagged)\n",
    "    df_grouped[['from_-1','from_1']] = df_grouped['from'].shift(periods = [-1,1])\n",
    "    df_grouped[['to_-1','to_1']] = df_grouped['to'].shift(periods = [-1,1])\n",
    "\n",
    "    #do the same for Subject_act: this will help me sift through cases where Trial expirations are conflicting with new trial or membership starts (some kind of rollover - ex. taylor.m.posey@outlook.com)\n",
    "    df_grouped[['Subject_act_-1','Subject_act_1']] = df_grouped['Subject_act'].shift(periods = [-1,1])\n",
    "\n",
    "\n",
    "    # Handle cases where the -1 or +1 shift are irrelevant: have to do with different email addresses, as detectable by the 'count' field (ie 1,2)\n",
    "    # This will actually be handled by a filter\n",
    "\n",
    "    df_grouped.reset_index(names = 'index', inplace= True)\n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/mofongo/Documents/ghfc/membershipReportsCIVI/membershipReportingLogicSampleReports')\n",
    "\n",
    "activityReport2 = pd.read_csv('./selectActivityReport_02092025.csv')\n",
    "\n",
    "activityReport2.columns = [i.replace(' ','_')+'_act' for i in list(activityReport2.columns)]\n",
    "\n",
    "activityReport2 = activityReport2.assign(Activity_Date_DT_act = pd.to_datetime(activityReport2['Activity_Date_act'], format = '%Y-%m-%d %H:%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityReport = activityReport2.drop_duplicates(ignore_index = True, subset = ['Activity_Type_act', 'Subject_act', 'Activity_Date_act', 'Activity_Status_act', 'Activity_Date_DT_act','Target_Email_act'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(546, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activityReport.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### SELECT BEST TRANSACTION LOGIC\n",
    "#NOTE: real dupes have multiple entries on fields: 'Target_Email_act','Activity_Date_DT_act','Activity_Type_act'\n",
    "#NOTE 'Activity_Type_act' = specifies if change is for status or type\n",
    "# looks like the index is preserved on a groupby\n",
    "df_grouped = activityReport.groupby(['Target_Email_act','Activity_Date_DT_act','Activity_Type_act']).filter(lambda x: len(x) > 1)\n",
    "\n",
    "#    except ValueError as e:\n",
    "#        raise ValueError(f'fields in this version of the dataframe do not coincide with the code in civiActivityReport_selectBestTrans.py: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign a row value to ea group member: used later for selection\n",
    "df_grouped['count'] = df_grouped.groupby(['Target_Email_act','Activity_Date_DT_act','Activity_Type_act']).cumcount()+1\n",
    "\n",
    "\n",
    "# Issues to handle:\n",
    "# - multiple records for different versions of Target_Name_act (yet Target_Email_act is the same) <- will need to check that this doesn't delete Family account members\n",
    "# - records made by different Source_Email_act ie systematic records made by CIVI\n",
    "\n",
    "df_grouped['from'] = df_grouped['Subject_act'].str.extract(r'from\\s(\\w+)') #str.extract(r'from\\s(\\w+)')\n",
    "df_grouped['to'] = df_grouped['Subject_act'].str.extract(r'to\\s(\\w+)') #str.extract(r'to\\s(\\w+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.sort_values(['Target_Email_act','Activity_Type_act','Activity_Date_DT_act'],inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Assignee_Name_act', 'Target_Name_act', 'Source_Email_act',\n",
       "       'Target_Email_act', 'Activity_Type_act', 'Subject_act',\n",
       "       'Activity_Date_act', 'Activity_Status_act', 'Activity_Details_act',\n",
       "       'Activity_Date_DT_act', 'count', 'from', 'to'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_grouped['from'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped['from'].shift(periods = [-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Columns must be same length as key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -1 = the row below (lead); +1 = the row above (lagged)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_grouped[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom_-1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom_1\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m df_grouped[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(periods \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/py12/lib/python3.12/site-packages/pandas/core/frame.py:4299\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_frame(key, value)\n\u001b[1;32m   4298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Series, np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mlist\u001b[39m, Index)):\n\u001b[0;32m-> 4299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array(key, value)\n\u001b[1;32m   4300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[1;32m   4301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item_frame_value(key, value)\n",
      "File \u001b[0;32m~/anaconda3/envs/py12/lib/python3.12/site-packages/pandas/core/frame.py:4341\u001b[0m, in \u001b[0;36mDataFrame._setitem_array\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4337\u001b[0m     \u001b[38;5;66;03m# Note: unlike self.iloc[:, indexer] = value, this will\u001b[39;00m\n\u001b[1;32m   4338\u001b[0m     \u001b[38;5;66;03m#  never try to overwrite values inplace\u001b[39;00m\n\u001b[1;32m   4340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[0;32m-> 4341\u001b[0m         check_key_length(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, key, value)\n\u001b[1;32m   4342\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k1, k2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(key, value\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[1;32m   4343\u001b[0m             \u001b[38;5;28mself\u001b[39m[k1] \u001b[38;5;241m=\u001b[39m value[k2]\n",
      "File \u001b[0;32m~/anaconda3/envs/py12/lib/python3.12/site-packages/pandas/core/indexers/utils.py:390\u001b[0m, in \u001b[0;36mcheck_key_length\u001b[0;34m(columns, key, value)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(key):\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns must be same length as key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# Missing keys in columns are represented as -1\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(key)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(value\u001b[38;5;241m.\u001b[39mcolumns):\n",
      "\u001b[0;31mValueError\u001b[0m: Columns must be same length as key"
     ]
    }
   ],
   "source": [
    "    # -1 = the row below (lead); +1 = the row above (lagged)\n",
    "df_grouped[['from_-1','from_1']] = df_grouped['from'].shift(periods = [-1,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_grouped[['to_-1','to_1']] = df_grouped['to'].shift(periods = [-1,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
