{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python file serves as an orchestration program to be run **after** *civiActivityReport.ipynb*.\n",
    "Components:\n",
    "1. make copies of the two output tables from civiActivityReport.ipynb in order to standardize them for the proceeding stored procedures. \n",
    "**original name convention**: \n",
    "- mem_status_(2-digit mo, 2-digit day) ex. \"mem_status_0406\" \n",
    "- mem_type_(2-digit mo, 2-digit day); ex \"mem_type_0406\"\n",
    "\n",
    "    This is done in order to preserve the original import table names, which should be deleted manually later\n",
    "\n",
    "2. run ea stored procedure (*stored_procedure_create_type_tables*, *stored_procedure_create_status_table*), ea of which serve to insert new records (output from the new CIVI import processed by *civiActivityReport.ipynb*) into a new version of the cumulative type and status tables of the db; consolidated table names in db: consolidated_mem_type, consolidated_mem_status\n",
    "3. conduct QA on the new version of the two consolidated output tables from the stored procedure: *consolidated_mem_type_temp2* and *consolidated_mem_status_temp2*\n",
    "4. if QA from #3 passes, replace the two prod *consolidated* tables\n",
    "5. call the stored procedure to create the stack_job table: *stored_procedure_create_stack_job.sql*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import itertools\n",
    "import json\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to the 172.17.0.2 for user root created successfully.\n"
     ]
    }
   ],
   "source": [
    "# DEFINE THE DATABASE CREDENTIALS\n",
    "user = 'root'\n",
    "password = 'baeldung'\n",
    "host = '172.17.0.2'\n",
    "port = 3306\n",
    "database = 'membership'\n",
    "\n",
    "def get_connection():\n",
    "\treturn sqlalchemy.create_engine(\n",
    "\t\turl=\"mysql+pymysql://{0}:{1}@{2}:{3}/{4}\".format(\n",
    "\t\t\tuser, password, host, port, database\n",
    "\t\t)\n",
    "\t)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\ttry:\n",
    "\t\n",
    "\t\t# GET THE CONNECTION OBJECT (ENGINE) FOR THE DATABASE\n",
    "\t\t# working w/engines: https://docs.sqlalchemy.org/en/20/core/engines_connections.html\n",
    "\t\tengine = get_connection() #engine should be created just once, and can manage several DBAPI connections\n",
    "\t\tprint(\n",
    "\t\t\tf\"Connection to the {host} for user {user} created successfully.\")\n",
    "\texcept Exception as ex:\n",
    "\t\tprint(\"Connection could not be made due to the following error: \\n\", ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make copies of the two output tables from the .ipynb <- change table name to a generic name to be consumed by the stored procedures\n",
    "def copy_rename(type_table: str, status_table: str):\n",
    "    #a CORE approach\n",
    "    #type_table = 'mem_type_'\n",
    "    #status_table = 'mem_status_'\n",
    "    #want to limit the scope of the of our use of this object to a specific context, so we use Python's context manager \"with\"\n",
    "    with engine.connect() as conn: #interacting w/db through Connection class\n",
    "        conn.execute(sqlalchemy.text(\"DROP TABLE IF EXISTS mem_type_new_import\"))\n",
    "        conn.execute(sqlalchemy.text(f\"CREATE TABLE mem_type_new_import LIKE {type_table}\"))\n",
    "        conn.execute(sqlalchemy.text(f\"INSERT INTO mem_type_new_import SELECT * FROM {type_table}\"))\n",
    "\n",
    "        conn.execute(sqlalchemy.text(\"DROP TABLE IF EXISTS mem_status_new_import\"))\n",
    "        conn.execute(sqlalchemy.text(f\"CREATE TABLE mem_status_new_import LIKE {status_table}\"))\n",
    "        conn.execute(sqlalchemy.text(f\"INSERT INTO mem_status_new_import SELECT * FROM {status_table}\"))\n",
    "        #conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if I want to handle errors, the relative error category is \"programming-time error\"\n",
    "copy_rename('mem_type_0406','mem_status_0406')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt to show all the tables of the db: https://docs.sqlalchemy.org/en/20/core/metadata.html; look around for db metadata\n",
    "#The MetaData object can also get a listing of tables and reflect the full set. This is achieved by using the reflect() method. \n",
    "from sqlalchemy import MetaData\n",
    "\n",
    "#this is incomplete, as inspect provided a good solution\n",
    "metadata_obj = MetaData()\n",
    "metadata_obj.reflect(bind=engine)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all clear\n"
     ]
    }
   ],
   "source": [
    "#inspector option: https://docs.sqlalchemy.org/en/20/core/reflection.html#fine-grained-reflection-with-inspector\n",
    "# inspector is a low level interface which provides a backend-agnostic system of loading lists of schema, table, column, and constraint descriptions from a given database is also available.\n",
    "from sqlalchemy import inspect\n",
    "insp = inspect(engine)\n",
    "table_name_list = insp.get_table_names()\n",
    "if all([i in table_name_list for i in('mem_type_0406','mem_status_0406')]):\n",
    "    print('all clear')\n",
    "else:\n",
    "    print('new tables from copy_rename() step aren\\'t found in db')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run each stored procedure; first check that the stored procedure is stored on the db (query\" *show procedure status where definer LIKE '%root%';*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GetStudentData', 'status_table_create', 'table_creations', 'typetablecreate', 'type_table_create']\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(sqlalchemy.text(\"show procedure status where definer LIKE '%root%'\"))\n",
    "    lista = [i[1] for i in result.all()]\n",
    "\n",
    "print(lista)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "#a Core approach (because I'm interacting explicitly w/the engine as opposed to abstracted objects), where I write explicit SQL code\n",
    "if all([i in lista for i in ['status_table_create', 'type_table_create']]):\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(sqlalchemy.text(\"call type_table_create()\"))\n",
    "        conn.execute(sqlalchemy.text(\"call status_table_create()\"))\n",
    "else:\n",
    "    print(\"stored procedures need to be compiled in server\") # running the stored procedure codebase script (.sql) from Python is an option\n",
    "    #attempting to run the .sql as scripts\n",
    "    with engine.connect() as conn:\n",
    "        with open(\"/home/candela/Documents/greeneHill/membershipReportsCIVI/github/greeneHill/stored_procedure_create_type_tables.sql\") as file:\n",
    "            query = text(file.read())\n",
    "            conn.execute(query)\n",
    "        with open(\"/home/candela/Documents/greeneHill/membershipReportsCIVI/github/greeneHill/stored_procedure_create_status_table.sql\") as file:\n",
    "            query = text(file.read())\n",
    "            conn.execute(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both key resultsets from the stored procedures verified in db\n"
     ]
    }
   ],
   "source": [
    "insp = inspect(engine)\n",
    "table_name_list = insp.get_table_names()\n",
    "\n",
    "# two key resultsets from the stored procedures: consolidated_mem_type_temp2 & consolidated_mem_status_temp2\n",
    "if all([i in table_name_list for i in ('consolidated_mem_type_temp2', 'consolidated_mem_status_temp2')]):\n",
    "    print('both key resultsets from the stored procedures verified in db')\n",
    "else:\n",
    "    print('stored procedures did not create the two key resultsets')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA options post stored procedure calling:\n",
    "- range of dates covered: new tables should extend **beyond** the legacy prod tables\n",
    "- \\# of total records, ie table size: new tables should have **more** records than legacy tables\n",
    "- analyze a contingency table of status or types: shape or dimension of contingency of new tables should be > or = to legacy\n",
    "\n",
    "The two stored procedures create persisted tables *consolidated_mem_type_temp2* and *consolidated_mem_status_temp2*. These serve as candidate tables to replace the prod tables *consolidated_mem_type* and *consolidated_mem_status*, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SELECT min(start_dt) FROM consolidated_mem_type_temp2;\n",
    "SELECT min(start_dt) FROM consolidated_mem_status_temp2;\n",
    "SELECT max(start_dt) FROM consolidated_mem_type_temp2;\n",
    "SELECT max(start_dt) FROM consolidated_mem_status_temp2;\n",
    "\n",
    "SELECT min(start_dt) FROM consolidated_mem_type;\n",
    "SELECT min(start_dt) FROM consolidated_mem_status;\n",
    "SELECT max(start_dt) FROM consolidated_mem_type;\n",
    "SELECT max(start_dt) FROM consolidated_mem_status;'''\n",
    "\n",
    "from sqlalchemy import MetaData # a CORE approach\n",
    "from sqlalchemy import Table\n",
    "from sqlalchemy.sql import select\n",
    "from sqlalchemy.sql import func\n",
    "metadata_obj = MetaData() # a container object\n",
    "#table reflection method to create a table object inferred from a table persisted in the db\n",
    "#ea of the below 4 tables are the results of the stored procedure run in the step above\n",
    "consolidated_mem_type_temp2 = Table(\"consolidated_mem_type_temp2\", metadata_obj, autoload_with=engine) # 'metadata_obj argument purpose is to associate the table to the metadata object\n",
    "consolidated_mem_status_temp2 = Table(\"consolidated_mem_status_temp2\", metadata_obj, autoload_with=engine)\n",
    "#pre-existing (to the calling of the stored procedures) consolidated tables\n",
    "consolidated_mem_type = Table(\"consolidated_mem_type\", metadata_obj, autoload_with=engine)\n",
    "consolidated_mem_status = Table(\"consolidated_mem_status\", metadata_obj, autoload_with=engine)\n",
    "\n",
    "\n",
    "with engine.connect() as conn: # Connections instances are typically for CORE and Sessions typical for ORM\n",
    "    #result = a CursorResult object; first() method returns a scalar\n",
    "    min_legacy_type = conn.execute(select(func.min(consolidated_mem_type.c.start_dt).label(\"minstart\"))).first()\n",
    "    max_legacy_type = conn.execute(select(func.max(consolidated_mem_type.c.start_dt).label(\"maxstart\"))).first()\n",
    "    min_replace_type = conn.execute(select(func.min(consolidated_mem_type_temp2.c.start_dt).label(\"maxstart\"))).first()\n",
    "    max_replace_type = conn.execute(select(func.max(consolidated_mem_type_temp2.c.start_dt).label(\"maxstart\"))).first()\n",
    "\n",
    "    min_legacy_status = conn.execute(select(func.min(consolidated_mem_status.c.start_dt).label(\"maxstart\"))).first()\n",
    "    max_legacy_status = conn.execute(select(func.max(consolidated_mem_status.c.start_dt).label(\"maxstart\"))).first()\n",
    "    min_replace_status = conn.execute(select(func.min(consolidated_mem_status_temp2.c.start_dt).label(\"maxstart\"))).first()\n",
    "    max_replace_status = conn.execute(select(func.max(consolidated_mem_status_temp2.c.start_dt).label(\"maxstart\"))).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, True, True]\n"
     ]
    }
   ],
   "source": [
    "#date ranges\n",
    "a = min_legacy_type == min_replace_type #start dates of legacy and \n",
    "b = max_legacy_type < max_replace_type\n",
    "\n",
    "c = min_legacy_status == min_replace_status\n",
    "d = max_legacy_status < max_replace_status\n",
    "\n",
    "print([a,b,c,d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that replacement table is longer than pre-existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(2024, 2, 17, 0, 0),), (datetime.datetime(2024, 2, 17, 0, 0),)]\n"
     ]
    }
   ],
   "source": [
    "print([max_legacy_type,max_replace_type])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
