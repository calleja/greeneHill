{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general testing module that uses unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import civiActivityReport_selectBestTrans as best\n",
    "import os\n",
    "import sys\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all of the names inside module best\n",
    "dir(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/mofongo/Documents/ghfc/membershipReportsCIVI/membershipReportingLogicSampleReports')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "activityReport = pd.read_csv('./selectActivityReport_10072024.csv')\n",
    "#activityReport.columns = [i.replace(' ','_')+'_act' for i in list(activityReport.columns)]\n",
    "activityReport.columns = [i.replace(' ','_')+'_act' for i in list(activityReport.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityReport.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "activityReport = activityReport.map(lambda x: x.strip() if isinstance(x,str) else x)\n",
    "activityReport = activityReport.assign(Activity_Date_DT_act = pd.to_datetime(activityReport['Activity_Date_act'], format = '%Y-%m-%d %H:%M'))\n",
    "activityReport.drop_duplicates(inplace=True, ignore_index = True, subset = ['Activity_Type_act', 'Subject_act', 'Activity_Date_act', 'Activity_Status_act', 'Activity_Date_DT_act','Target_Email_act'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#write out all the functions that return a boolean, which I used to test truthiness on my TestBest array of assertTrue functions\n",
    "df = pd.DataFrame()\n",
    "isinstance(df,pd.core.frame.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityReport.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_apply_exclusion (__main__.TestBest.test_apply_exclusion) ... ok\n",
      "test_apply_rules (__main__.TestBest.test_apply_rules) ... ok\n",
      "test_intake_file (__main__.TestBest.test_intake_file) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 1.612s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x752f20ded7c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestBest(unittest.TestCase):\n",
    "    \n",
    "    def test_intake_file(self):\n",
    "        df = best.intake_file(activityReport)\n",
    "        self.assertTrue(isinstance(df,pd.core.frame.DataFrame),'result of intake_file is not a df')\n",
    "\n",
    "        self.assertTrue(len(df) > 0,'df from intake_file is not adequate size')\n",
    "\n",
    "    def test_apply_rules(self):\n",
    "        df = best.intake_file(activityReport)\n",
    "        ser = best.apply_rules(df)\n",
    "        #len(ser) > 0\n",
    "        #df.shape[0] > len(ser)\n",
    "        self.assertTrue(len(ser) > 0,f'the DataFrame is not longer than Series')\n",
    "\n",
    "    def test_apply_exclusion(self):\n",
    "        df_group = best.intake_file(activityReport)\n",
    "        ser = best.apply_rules(df_group)\n",
    "        #function signature: apply_exclusion(inclusion_ser,df_grouped,activityReport)\n",
    "        result_df = best.apply_exclusion(ser,df_group,activityReport)\n",
    "        self.assertTrue(activityReport.shape[0] > result_df.shape[0])\n",
    "    \n",
    "    def test_remove_controller(self):\n",
    "        best.remove_controller(activityReport)\n",
    "        \n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def apply_exclusion(inclusion_ser,df_grouped,activityReport):\n",
      "    #inclusion_df = concat_grouped_ser from apply_rules()\n",
      "    #exclusion_list = a list - all indices of df_grouped OUTSIDE of those we want to keep (ie contact_grouped_df <- the cumulative DF)\n",
      "\n",
      "    #check that all expected columns exist\n",
      "    if ('index' in df_grouped.columns) and ('index' in inclusion_ser.columns):\n",
      "        exclusion_list = df_grouped.loc[~df_grouped['index'].isin(inclusion_ser['index']),'index'].to_list()\n",
      "        #isolate the records to remove by negative indexing on concat_grouped_df\n",
      "\n",
      "        #TODO inverse index activityReport by selecting indices NOT IN exclusion_list\n",
      "        #inspection\n",
      "        #df_grouped.loc[~df_grouped['index'].isin(concat_grouped_df['index']),:].sort_values(['Target_Email_act','Activity_Date_DT_act']).to_csv('/home/mofongo/Documents/ghfc/membershipReportsCIVI/records_to_remove.csv', index = False)\n",
      "        #test = activityReport.loc[~df_grouped.loc[~df_grouped['index'].isin(concat_grouped_df['index']),'index'],:]\n",
      "        test = activityReport.index.difference(exclusion_list)\n",
      "        filtered_df = activityReport.loc[test,:]\n",
      "        return filtered_df.drop_duplicates(ignore_index = True)\n",
      "    else:\n",
      "        raise ValueError(f\"either of the two dfs in apply_exclusion() do not include the required 'index' column\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(best.apply_exclusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def intake_file(activityReport):\n",
      "\n",
      "    #I removed some data munging here, as this should be done in another module or the calling ipynb; munging removed denoted w/ \"*\"\n",
      "\n",
      "    #activityReport.columns = [i.replace(' ','_')+'_act' for i in list(activityReport.columns)]*\n",
      "\n",
      "    #trim all string fields\n",
      "    #strip all whitespace from each cell\n",
      "    #activityReport = activityReport.map(lambda x: x.strip() if isinstance(x,str) else x) *\n",
      "\n",
      "    #NOTE: Activity_Date_act field DOES NOT provide seconds\n",
      "    #activityReport = activityReport.assign(Activity_Date_DT_act = pd.to_datetime(activityReport['Activity_Date_act'], format = '%Y-%m-%d %H:%M')) *\n",
      "\n",
      "    try:\n",
      "        activityReport = activityReport.drop_duplicates(ignore_index = True, subset = ['Activity_Type_act', 'Subject_act', 'Activity_Date_act', 'Activity_Status_act', 'Activity_Date_DT_act','Target_Email_act'])\n",
      "\n",
      "        # ### SELECT BEST TRANSACTION LOGIC\n",
      "        #NOTE: real dupes have multiple entries on fields: 'Target_Email_act','Activity_Date_DT_act','Activity_Type_act'\n",
      "        #NOTE 'Activity_Type_act' = specifies if change is for status or type\n",
      "        # looks like the index is preserved on a groupby\n",
      "        df_grouped = activityReport.groupby(['Target_Email_act','Activity_Date_DT_act','Activity_Type_act']).filter(lambda x: len(x) > 1)\n",
      "\n",
      "    except ValueError as e:\n",
      "        raise ValueError(f'fields in this version of the dataframe do not coincide with the code in civiActivityReport_selectBestTrans.py: {e}')\n",
      "\n",
      "\n",
      "    #assign a row value to ea group member: used later for selection\n",
      "    df_grouped['count'] = df_grouped.groupby(['Target_Email_act','Activity_Date_DT_act','Activity_Type_act']).cumcount()+1\n",
      "\n",
      "\n",
      "    # Issues to handle:\n",
      "    # - multiple records for different versions of Target_Name_act (yet Target_Email_act is the same) <- will need to check that this doesn't delete Family account members\n",
      "    # - records made by different Source_Email_act ie systematic records made by CIVI\n",
      "\n",
      "    df_grouped['from'] = df_grouped['Subject_act'].str.extract(r'from\\s(\\w+)') #str.extract(r'from\\s(\\w+)')\n",
      "    df_grouped['to'] = df_grouped['Subject_act'].str.extract(r'to\\s(\\w+)') #str.extract(r'to\\s(\\w+)')\n",
      "\n",
      "\n",
      "    # The record to keep is that where the \"From\" = \"To\" of the companion line of the group. There are only at most two entries with the same Start_dt, so I only need to worry about passing back the derived \"To\" and \"From\" fields twice. Either of row = 1 or row = 2 can be the best one to keep. Essentially I'm searching for the best determinite record of the status going forward, ignoring the journey (implying that accurate status is more important than accurate/comprehensive journey).\n",
      "    # \n",
      "    # Will need to offset forwards and backwards. Ea row will then have a pair of offset values (from the row above and the one beneath). Depending on the \"count\" value (ie 1,2), only one from the pair will be relevant and tested to determine the \"best\" and \"final\" value.\n",
      "\n",
      "    df_grouped.sort_values(['Target_Email_act','Activity_Type_act','Activity_Date_DT_act'],inplace= True)\n",
      "    # -1 = the row below (lead); +1 = the row above (lagged)\n",
      "    df_grouped[['from_-1','from_1']] = df_grouped['from'].shift(periods = [-1,1])\n",
      "    df_grouped[['to_-1','to_1']] = df_grouped['to'].shift(periods = [-1,1])\n",
      "\n",
      "    #do the same for Subject_act: this will help me sift through cases where Trial expirations are conflicting with new trial or membership starts (some kind of rollover - ex. taylor.m.posey@outlook.com)\n",
      "    df_grouped[['Subject_act_-1','Subject_act_1']] = df_grouped['Subject_act'].shift(periods = [-1,1])\n",
      "\n",
      "\n",
      "    # Handle cases where the -1 or +1 shift are irrelevant: have to do with different email addresses, as detectable by the 'count' field (ie 1,2)\n",
      "    # This will actually be handled by a filter\n",
      "\n",
      "    df_grouped.reset_index(names = 'index', inplace= True)\n",
      "    return df_grouped\n",
      "\n",
      "    #ideally just delete the irrelevant records: will need to identify the 'keep' and 'delete' records, specifically relying on index\n",
      "    #CASE WHERE ROW NUMBER =1 THEN WE DON'T CARE ABOUT LAGGED DATA (ie only pertinent data is suffix = -1)\n",
      "    #CASE WHERE ROW NUMBER =2 THEN WE DON'T CARE ABOUT LEAD DATA (ie only pertinent data is suffix = 1)\n",
      "    #REVERSING TRANSACTIONS: DELETE/DROP BOTH\n",
      "    #an example\n",
      "    #df_grouped.loc[df_grouped['Activity_Type_act'] != 'Membership Signup',['Target_Email_act','Activity_Date_DT_act','Subject_act','Subject_act_-1','Subject_act_1','count','from', 'to', 'from_-1', 'from_1','to_-1', 'to_1']].head(5)\n",
      "    # CASE WHERE ROW NUMBER = 1 AND to_-1 from -1\n",
      "\n",
      "\n",
      "    # Select logic in words:\n",
      "    # - if row = 1 THEN real case: from_-1 <> to AND to_-1 = from (choose this row and discard where row = 2)\n",
      "    # - if row = 2 THEN real case: to_1 = from AND from_1 <> to (choose this row and discard where row = 1)\n",
      "    # - case where two status updates are made at the same time that don't conform to the above two logic statements: choose whichever row DOES NOT contain \"Expired\"\n",
      "    # - if Activity_Type_act = 'Membership Signup' THEN choose the row where Subject_act does not contain \"Expired\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(best.intake_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
